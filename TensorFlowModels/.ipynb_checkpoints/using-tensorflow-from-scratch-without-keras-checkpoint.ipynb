{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TensorFlow is based on (surprise!) tensors. Tensors are veeery similar to numpy arrays. The main differences are that\n",
    "1. They can be easily handed by GPUs\n",
    "2. They are immutables\n",
    "I you want to know more, the official documentation has a very interesting [notebook](https://www.tensorflow.org/guide/tensor)\n",
    "showing their properties. You may have a look at the TensorFlow basics [guide](https://www.tensorflow.org/guide). This notebook is loosely based on that guide.\n",
    "\n",
    "We are going to use tensors to build a neural network from scratch. If you are relatively new to neural networks and don't\n",
    "know the mathematical details of how neural networks work, I recommend the introductory neural network lectures from Stanford\n",
    "University (here [part 1](https://www.youtube.com/watch?v=MfIjxPh6Pys) and [part 2](https://www.youtube.com/watch?v=zUazLXZZA2U)).\n",
    "From now on, I am going to assume that you know all the math background.\n",
    "\n",
    "OK, let's get our hands dirty! Let's start importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 20:01:42.668291: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-21 20:01:42.668318: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-21 20:01:43.501737: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 20:01:43.501846: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 20:01:43.501857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder, scale\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 20:01:44.877532: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-02-21 20:01:44.877593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (archlinux): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "X_train=np.linspace(0,2*np.pi,10)\n",
    "target_y = tf.sin(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And prepare it for our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done with the preparation!\n",
    "\n",
    "# Neural network without hidden layers\n",
    "\n",
    "Now let's start with a very simple model of one layer. First we need a weight ~~matrix~~ tensor.\n",
    "We have 784 inputs (pixels) and 10 outputs (numbers 0-9), therefore the weight matrix should be 784 x 10. Let's initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_random = tf.Variable(tf.zeros([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are using a *variable* instead of a tensor. Variables are essentially tensors wrapped in something more, in\n",
    "order to allow them to do fancy things, so\n",
    "for the time being let's consider them equivalent.\n",
    "\n",
    "Let's define a couple of well-known metrics: the mean squared error loss and the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(target_y, predicted_y):\n",
    "  return tf.reduce_mean(tf.square(target_y - predicted_y))\n",
    "\n",
    "def loss_crsntrpy(target_y,predicted_y):\n",
    "  return -tf.reduce_sum(tf.reduce_mean(target_y * tf.math.log(predicted_y + 1e-12),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the names `reduce_sum`, `reduce_mean` reminds you to Hadoop's MapReduce. You are not [totally wrong](https://realpython.com/python-reduce-function/)\n",
    "\n",
    "Although we can build our simple neural network model using the weights_random variable and loss_crsntrpy alone, it is\n",
    "simpler to use some of the TensorFlow machinery already there for us. One is the [tf.Module](https://www.tensorflow.org/api_docs/python/tf/Module)\n",
    "object. It is very convenient\n",
    "to define classes based on tf.Module, because we don't need to implement things like tf.Variable handling.\n",
    "Our model will be a class derived from tf.Module, where we will define the weights matrix `w` and\n",
    "the matrix of biases `b` as class properties. We make the class callable, so we can make predictions just doing `y_predicted = MyModel()(X_model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.Module):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    # Let's start this to zeros. We will see soon why this is not a good idea.\n",
    "    self.w = tf.Variable(tf.zeros([1]))\n",
    "    self.b = tf.Variable(0.0)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return tf.nn.softmax(x @ self.w + self.b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define the train function. Note that we use this strange class called GradientType: what is this and how\n",
    "does it work? If you watched the Stanford University lectures, you know that the goal is to minimize the loss, and for that we\n",
    "need to calculate the derivative of the loss with respect to our weights. How can we calculate derivatives with a computer?\n",
    "There are three possibilities, [numerical differentation](https://en.wikipedia.org/wiki/Numerical_differentiation),\n",
    "[symbolic differentation](https://en.wikipedia.org/wiki/Computer_algebra) and [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
    "Numerical differentiation is the easiest to understand and implement. The problem is that is not accurate enough for deep\n",
    "neural networks. Symbolic differentiation leads to very complicated expressions (and may not be able to handle certain\n",
    "layers). Automatic differentiation is somehow in the middle and is the best way of calculate derivatives of neural networks\n",
    "(thanks to its clever use of the chain rule).\n",
    "\n",
    "OK, but what is gradient tape? In order to calculate the derivative, TensorFlow should somehow keep track of the functions\n",
    "that we are defining and that we want to differentiate, and the variables with respect to we want to differentiate (now you start to see why `tf.Variable` is useful, don't you?). GradientTape is the class that is used for that. You can read more\n",
    "details in the [official documentation](https://www.tensorflow.org/api_docs/python/tf/GradientTape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,x,y,learning_rate):\n",
    "\n",
    "  with tf.GradientTape() as t:\n",
    "    current_loss = loss_crsntrpy(y, model(x))\n",
    "\n",
    "  dw, db = t.gradient(current_loss, [model.w, model.b])\n",
    "\n",
    "  # assign_sub is a function of the variable class that does w = w - (learning_rate * dw) in an efficient way\n",
    "  model.w.assign_sub(learning_rate * dw)\n",
    "  model.b.assign_sub(learning_rate * db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_epochs in range(50):\n",
    "  train(model,X_train,target_y,learning_rate=0.2)\n",
    "  train_loss = loss_crsntrpy(target_y,model(X_train))\n",
    " # test_loss = loss_crsntrpy(y_test,model(X_test))\n",
    "  #print(\"Training loss in epoch {0} = {1}.   Test loss = {2}\".format(i_epochs,train_loss.numpy(),test_loss.numpy()))\n",
    "  print(\"Training loss in epoch {0} = {1}.   Test loss = {2}\".format(i_epochs,train_loss.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we got there! We have a cross-entropy loss of 0.37, that is not bad at all.\n",
    "\n",
    "# Neural network with a hidden layer\n",
    "\n",
    "Now let's include a hidden layer of 100 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHidden(tf.Module):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    # Initialize the weights to `5.0` and the bias to `0.0`\n",
    "    # In practice, these should be randomly initialized\n",
    "    self.w0 = tf.Variable(tf.zeros([784,100]))\n",
    "    self.b0 = tf.Variable(0.0)\n",
    "\n",
    "    self.w1 = tf.Variable(tf.zeros([100,10]))\n",
    "    self.b1 = tf.Variable(0.0)\n",
    "\n",
    "  def __call__(self, x0):\n",
    "    x1 = tf.nn.sigmoid(x0 @ self.w0 + self.b0)\n",
    "    return tf.nn.softmax(x1 @ self.w1 + self.b1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a new train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hidden(model,x,y,learning_rate):\n",
    "\n",
    "  with tf.GradientTape() as t:\n",
    "    current_loss = loss_crsntrpy(y, model(x))\n",
    "\n",
    "  dw1, dw0, db1, db0 = t.gradient(current_loss, [model.w1, model.w0, model.b1, model.b0])\n",
    "\n",
    "  model.w1.assign_sub(learning_rate * dw1)\n",
    "  model.b1.assign_sub(learning_rate * db1)\n",
    "  model.w0.assign_sub(learning_rate * dw0)\n",
    "  model.b0.assign_sub(learning_rate * db0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hidden = ModelHidden()\n",
    "\n",
    "for i_epochs in range(50):\n",
    "  train_hidden(model_hidden,X_train,y_train,learning_rate=0.2)\n",
    "  train_loss = loss_crsntrpy(y_train,model_hidden(X_train))\n",
    "  test_loss = loss_crsntrpy(y_test,model_hidden(X_test))\n",
    "  print(\"Training loss in epoch {0} = {1}.   Test loss = {2}\".format(i_epochs,train_loss.numpy(),test_loss.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network seems to not be learning anything. This is because we initialize all the weights to zero. This way, the\n",
    "neurons are heavily correlated and in practice, they behave as one neuron. One of the things of working with TensorFlow at\n",
    "low level is that we can easily have a look at the weight tensor. We can see that the weight of all neurons are almost the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_hidden.w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get rid of this initializing the weight matrix to random numbers.\n",
    "(We will soon see that this may be not enough for more complex architectures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterModelHidden(tf.Module):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.w0 = tf.Variable(tf.random.uniform([784, 100]))\n",
    "    self.b0 = tf.Variable(0.0)\n",
    "\n",
    "    self.w1 = tf.Variable(tf.random.uniform([100, 10]))\n",
    "    self.b1 = tf.Variable(0.0)\n",
    "\n",
    "  def __call__(self, x0):\n",
    "    x1 = tf.nn.sigmoid(x0 @ self.w0 + self.b0)\n",
    "    return tf.nn.softmax(x1 @ self.w1 + self.b1)\n",
    "\n",
    "model_hidden = BetterModelHidden()\n",
    "\n",
    "for i_epochs in range(50):\n",
    "  train_hidden(model_hidden, X_train, y_train, learning_rate=0.2)\n",
    "  train_loss = loss_crsntrpy(y_train, model_hidden(X_train))\n",
    "  test_loss = loss_crsntrpy(y_test, model_hidden(X_test))\n",
    "  print(\"Training loss in epoch {0} = {1}.   Test loss = {2}\".format(i_epochs, train_loss.numpy(), test_loss.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it works better! It seems that its loss is larger than the no-hidden layer neural network, but this is just because the model is still not converged (gradient descent is not the most efficient algorithm). If we train both neural networks with a better algorithm (or more than 2000 epochs) we will see that the model with a hidden layer is indeed better.\n",
    "\n",
    "# Deep neural networks and weight initialization strategies\n",
    "\n",
    "We were able to get relatively good results with the two-layer neural network. But what if we want to implement a complex architecture\n",
    "like [these](http://slazebni.cs.illinois.edu/spring17/lec01_cnn_architectures.pdf)?\n",
    "Our code soon becomes too complex and error prone. For that reason we are going to create a new `MyLayer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(tf.Module):\n",
    "  def __init__(self,input_size,output_size,is_last=False, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.w = tf.Variable(tf.random.uniform([input_size,output_size]))\n",
    "    self.b = tf.Variable(0.0)\n",
    "    self.is_last = is_last\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if self.is_last:\n",
    "      result = tf.nn.softmax(x @ self.w + self.b)\n",
    "    else:\n",
    "      result = tf.nn.sigmoid(x @ self.w + self.b)\n",
    "    return result\n",
    "\n",
    "\n",
    "class MyNeuralNetwork(tf.Module):\n",
    "  def __init__(self,layers,**kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "    # Check if layers sizes are inconsistent\n",
    "    first_layer = True\n",
    "    for layer in layers:\n",
    "      if first_layer:\n",
    "        first_layer = False\n",
    "      else:\n",
    "        if layer.input_size != previous_output:\n",
    "          print('Inconsistent layers')\n",
    "      previous_output = layer.output_size\n",
    "    if layers[-1].is_last == False:\n",
    "      print('Last layer is_last = True!')\n",
    "    self._layers = layers\n",
    "\n",
    "  def __call__(self, x0):\n",
    "    for i_layer in self._layers:\n",
    "      x0 = i_layer(x0)\n",
    "    return x0\n",
    "\n",
    "def train_nn(neural_network,x,y,learning_rate):\n",
    "\n",
    "  with tf.GradientTape(persistent=True) as t:\n",
    "    current_loss = loss_crsntrpy(y, neural_network(x))\n",
    "\n",
    "  gradiente = t.gradient(current_loss, neural_network.trainable_variables)\n",
    "\n",
    "  for i_trainable_variable,i_gradient in zip(neural_network.trainable_variables,gradiente):\n",
    "    i_trainable_variable.assign_sub(learning_rate*i_gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a huge neural network and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MyNeuralNetwork([MyLayer(784,500),MyLayer(500,500),MyLayer(500,500),\n",
    "                      MyLayer(500,500),MyLayer(500,500),MyLayer(500,500),\n",
    "                      MyLayer(500,10,is_last=True)])\n",
    "\n",
    "for i_epochs in range(50):\n",
    "  train_nn(nn,X_train,y_train,learning_rate=0.2)\n",
    "  train_loss = loss_crsntrpy(y_train,nn(X_train))\n",
    "  test_loss = loss_crsntrpy(y_test,nn(X_test))\n",
    "  print(\"Training loss in epoch {0} = {1}.   Test loss = {2}\".format(i_epochs,train_loss.numpy(),test_loss.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mmmhhh... what happened here? The loss is definitely not improving! Why using a larger neural network leads to worse\n",
    "results? The answer is actually quite easy. Let's have a look at the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as t:\n",
    "  current_loss = loss_crsntrpy(y_train, nn(X_train))\n",
    "\n",
    "check_gradient = t.gradient(current_loss, nn.trainable_variables)\n",
    "print(check_gradient[1])\n",
    "# Add (the absolute value of) all elements\n",
    "print(np.sum(np.abs(check_gradient[1].numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is zero everywhere ($\\sum_{ij} |g_{ij}| = 0$)! What is going on here? In fact this is a well-known problem.\n",
    "The problem of vanishing gradients. It appears because the typical sigmoid activation\n",
    "function tends to saturate (i. e. $\\sigma(x) \\approx \\sigma(x + \\Delta x)$ for $x > 3$). In other words, the\n",
    "sigmoid won't change with changes in x, therefore derivatives (gradients) are zero. We\n",
    "can see this problem here in full detail (advantages of being working with TensorFlow at a low level).\n",
    "Let's see what happen in the linear part of the first layer (before applying the sigmoid function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10,10,200),tf.sigmoid(np.linspace(-10,10,200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train @ nn._layers[0].w + nn._layers[0].b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after the sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.nn.sigmoid(X_train @ nn._layers[0].w + nn._layers[0].b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got mostly either 0s or 1s. That means that our first layer is either completely saturated or unsaturated for almost\n",
    "every input. No surprise why the gradients are zero.\n",
    "How can we do to improve it? There are better ways to initialize weights than a random number between zero and one.\n",
    "See this 30-min long TowardsDataScience [article](https://towardsdatascience.com/weight-initialization-in-deep-neural-networks-268a306540c0) (behind paywall) for a large and thorough discussion of different weight initialization techniques,\n",
    "or just skip the book and go for the [movie](https://youtu.be/zUazLXZZA2U?t=3243).\n",
    "\n",
    "Let's create a new layer class using the so-called Xavier initialization scheme for the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBetterLayer(MyLayer):\n",
    "  def __init__(self,input_size,output_size,is_last = False,**kwargs):\n",
    "    super().__init__(input_size,output_size,is_last,**kwargs)\n",
    "    self.is_last = is_last\n",
    "    self.w = tf.Variable(tf.random.normal([input_size, output_size]) * tf.sqrt(2 / (input_size + output_size)), name='w')\n",
    "    self.b = tf.Variable(0.0,name='b')\n",
    "    \n",
    "nn = MyNeuralNetwork([MyBetterLayer(784,500),MyBetterLayer(500,500),MyBetterLayer(500,500),\n",
    "                      MyBetterLayer(500,500),MyBetterLayer(500,500),MyBetterLayer(500,500),\n",
    "                      MyBetterLayer(500,10,is_last=True)])\n",
    "\n",
    "for i_epochs in range(50):\n",
    "  train_nn(nn,X_train,y_train,learning_rate=0.2)\n",
    "  train_loss = loss_crsntrpy(y_train,nn(X_train))\n",
    "  test_loss = loss_crsntrpy(y_test,nn(X_test))\n",
    "  print(\"Training loss in epoch {0} = {1}.   Test loss = {2}\".format(i_epochs,train_loss.numpy(),test_loss.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the gradient is much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as t:\n",
    "  current_loss = loss_crsntrpy(y_train, nn(X_train))\n",
    "\n",
    "check_gradient = t.gradient(current_loss, nn.trainable_variables)\n",
    "print(check_gradient[1])\n",
    "print(np.sum(np.abs(check_gradient[1].numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final remarks\n",
    "\n",
    "With TensorFlow we can choose if we want the tensors to be run in the GPU or in the CPU. Let's see the difference in time, just to see the advantage of GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "nn = MyNeuralNetwork([MyBetterLayer(784,500),MyBetterLayer(500,500),MyBetterLayer(500,500),\n",
    "                      MyBetterLayer(500,500),MyBetterLayer(500,500),MyBetterLayer(500,500),\n",
    "                      MyBetterLayer(500,10,is_last=True)])\n",
    "\n",
    "for i_epochs in range(50):\n",
    "  train_nn(nn,X_train,y_train,learning_rate=0.2)\n",
    "end_time = time()\n",
    "\n",
    "print('Using a GPU we needed {0} seconds to train the network'.format(end_time-start_time))\n",
    "\n",
    "\n",
    "with tf.device('CPU:0'):\n",
    "  start_time = time()\n",
    "  nn = MyNeuralNetwork([MyBetterLayer(784, 500), MyBetterLayer(500, 500), MyBetterLayer(500, 500),\n",
    "                        MyBetterLayer(500, 500), MyBetterLayer(500, 500), MyBetterLayer(500, 500),\n",
    "                        MyBetterLayer(500, 10, is_last=True)])\n",
    "\n",
    "  for i_epochs in range(50):\n",
    "    train_nn(nn, X_train, y_train, learning_rate=0.2)\n",
    "  end_time = time()\n",
    "\n",
    "print('Using a CPU we needed {0} seconds to train the network'.format(end_time-start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can slightly change the neural network object to make it more Keras-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MySequential(tf.Module):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self._layers = []\n",
    "\n",
    "  def add(self,layer):\n",
    "    self._layers.append(layer)\n",
    "\n",
    "  def fit(self,x,y,learning_rate,n_epochs):\n",
    "    for i_epoch in range(n_epochs):\n",
    "      with tf.GradientTape(persistent=True) as t:\n",
    "        current_loss = loss_crsntrpy(y, self.predict(x))\n",
    "      gradiente = t.gradient(current_loss, self.trainable_variables)\n",
    "\n",
    "      for i_trainable_variable, i_gradient in zip(self.trainable_variables, gradiente):\n",
    "        i_trainable_variable.assign_sub(learning_rate * i_gradient)\n",
    "\n",
    "      train_loss = loss_crsntrpy(y_train, nn.predict(X_train))\n",
    "      test_loss = loss_crsntrpy(y_test, nn.predict(X_test))\n",
    "      print(\"Training loss in epoch {0} = {1}.   Test loss = {2}\".format(i_epoch, train_loss.numpy(), test_loss.numpy()))\n",
    "\n",
    "  def predict(self,x0):\n",
    "    for i_layer in self._layers:\n",
    "      x0 = i_layer(x0)\n",
    "    return x0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MySequential()\n",
    "\n",
    "nn.add(MyBetterLayer(784,100))\n",
    "nn.add(MyBetterLayer(100,100))\n",
    "nn.add(MyBetterLayer(100,10,is_last=True))\n",
    "\n",
    "nn.fit(X_train,y_train,0.2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it! Now you can create your own layers! However, this simple high level API of TensorFlow that we have\n",
    "created lacks a lot of features that are already in Keras (for instance, better algorithms for loss minimization). Fortunately, Keras API has its own [Layer](https://keras.io/api/layers/base_layer/) object that you\n",
    "can easily [extend](https://www.tensorflow.org/tutorials/customization/custom_layers) to create your own layer. This way you can create layers not present in Keras, while taking advantage\n",
    "of the other parts of the Keras ecosystem.\n",
    "\n",
    "Happy coding!\n",
    "\n",
    "\n",
    "\n",
    "![](https://cdn.pixabay.com/photo/2014/10/22/23/14/geek-499140_960_720.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
